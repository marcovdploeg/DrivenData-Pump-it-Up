{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook contains the code to generate the submission for the \"Pump it Up: Data Mining the Water Table\" competition.\n",
    "\n",
    "We use the preprocessed training data and corresponding values, as well as test data. We need to predict the ordinal variable 'status_group', with values 0, 1, 2. The error metric used in the competition is the classification rate (fraction of predictions that are correct).\n",
    "\n",
    "In this script we train a tensorflow deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "tf.random.set_seed(42)  # For reproducibility, but it only works to like 2 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../prep_data/X_train.csv')\n",
    "y_train = pd.read_csv('../prep_data/y_train.csv')\n",
    "X_val = pd.read_csv('../prep_data/X_val.csv')\n",
    "y_val = pd.read_csv('../prep_data/y_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "n_units = 64\n",
    "n_layers = 2\n",
    "model = keras.Sequential()\n",
    "# Add the first layer based on input shape\n",
    "model.add(layers.Dense(n_units, activation=\"relu\", input_shape=[X_train.shape[1]]))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.BatchNormalization())\n",
    "# Add the remaining layers\n",
    "for _ in range(n_layers-1):\n",
    "    model.add(layers.Dense(n_units, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.BatchNormalization())\n",
    "# Add the output layer, for classification 0,1,2\n",
    "model.add(layers.Dense(3, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Set early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=512,\n",
    "    epochs=1000,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372/372 [==============================] - 1s 2ms/step\n",
      "Classification rate: 0.7425084175084176\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_val)\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "# Translate the one-hot encoding to the class\n",
    "y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "class_rate = np.mean(y_pred_class == y_val.values.ravel())\n",
    "print(f\"Classification rate: {class_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372/372 [==============================] - 1s 2ms/step\n",
      "n_units: 64, n_layers: 1, class_rate: 0.7511784511784512\n",
      "372/372 [==============================] - 1s 2ms/step\n",
      "n_units: 64, n_layers: 2, class_rate: 0.7563973063973064\n",
      "372/372 [==============================] - 1s 2ms/step\n",
      "n_units: 64, n_layers: 3, class_rate: 0.7442760942760943\n",
      "372/372 [==============================] - 1s 2ms/step\n",
      "n_units: 64, n_layers: 4, class_rate: 0.7510942760942761\n",
      "372/372 [==============================] - 1s 1ms/step\n",
      "n_units: 128, n_layers: 1, class_rate: 0.7548821548821549\n",
      "372/372 [==============================] - 1s 2ms/step\n",
      "n_units: 128, n_layers: 2, class_rate: 0.7452861952861953\n",
      "372/372 [==============================] - 1s 2ms/step\n",
      "n_units: 128, n_layers: 3, class_rate: 0.7497474747474747\n",
      "372/372 [==============================] - 1s 2ms/step\n",
      "n_units: 128, n_layers: 4, class_rate: 0.7474747474747475\n",
      "372/372 [==============================] - 1s 2ms/step\n",
      "n_units: 256, n_layers: 1, class_rate: 0.7527777777777778\n",
      "372/372 [==============================] - 1s 2ms/step\n",
      "n_units: 256, n_layers: 2, class_rate: 0.7582491582491583\n",
      "372/372 [==============================] - 1s 2ms/step\n",
      "n_units: 256, n_layers: 3, class_rate: 0.7538720538720539\n",
      "372/372 [==============================] - 1s 2ms/step\n",
      "n_units: 256, n_layers: 4, class_rate: 0.7592592592592593\n",
      "372/372 [==============================] - 1s 2ms/step\n",
      "n_units: 512, n_layers: 1, class_rate: 0.7487373737373737\n",
      "372/372 [==============================] - 1s 2ms/step\n",
      "n_units: 512, n_layers: 2, class_rate: 0.7489057239057239\n",
      "372/372 [==============================] - 1s 2ms/step\n",
      "n_units: 512, n_layers: 3, class_rate: 0.7536195286195286\n",
      "372/372 [==============================] - 1s 3ms/step\n",
      "n_units: 512, n_layers: 4, class_rate: 0.757996632996633\n",
      "0.7592592592592593\n"
     ]
    }
   ],
   "source": [
    "# Optimise\n",
    "def fit_model(n_units, n_layers):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(n_units, activation=\"relu\", input_shape=[X_train.shape[1]]))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    for _ in range(n_layers-1):\n",
    "        model.add(layers.Dense(n_units, activation=\"relu\"))\n",
    "        model.add(layers.Dropout(0.3))\n",
    "        model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(3, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=512,\n",
    "        epochs=1000,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred = np.round(y_pred)\n",
    "    y_pred_class = np.argmax(y_pred, axis=1)\n",
    "    class_rate = np.mean(y_pred_class == y_val.values.ravel())\n",
    "    return class_rate\n",
    "\n",
    "param_grid = {\n",
    "    'n_units': [64, 128, 256, 512],\n",
    "    'n_layers': [1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "scores = []\n",
    "for n_units in param_grid['n_units']:\n",
    "    for n_layers in param_grid['n_layers']:\n",
    "        class_rate = fit_model(n_units, n_layers)\n",
    "        print(f\"n_units: {n_units}, n_layers: {n_layers}, class_rate: {class_rate}\")\n",
    "        scores.append(class_rate)\n",
    "print(max(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372/372 [==============================] - 1s 2ms/step\n",
      "n_units: 256, n_layers: 5, class_rate: 0.757996632996633\n",
      "372/372 [==============================] - 1s 2ms/step\n",
      "n_units: 256, n_layers: 6, class_rate: 0.757996632996633\n",
      "0.757996632996633\n"
     ]
    }
   ],
   "source": [
    "# The best score was achieved with n_units=256 and n_layers=4, by a small margin\n",
    "# Let's then also check if adding even more layers helps\n",
    "param_grid = {\n",
    "    'n_units': [256],\n",
    "    'n_layers': [5, 6]\n",
    "}\n",
    "\n",
    "scores = []\n",
    "for n_units in param_grid['n_units']:\n",
    "    for n_layers in param_grid['n_layers']:\n",
    "        class_rate = fit_model(n_units, n_layers)\n",
    "        print(f\"n_units: {n_units}, n_layers: {n_layers}, class_rate: {class_rate}\")\n",
    "        scores.append(class_rate)\n",
    "print(max(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372/372 [==============================] - 1s 2ms/step\n",
      "Classification rate: 0.7587542087542087\n"
     ]
    }
   ],
   "source": [
    "# Seems like n_units=256 and n_layers=4 is still the best, but the difference is small\n",
    "# Let's then use this model to predict the test data\n",
    "model_fin = keras.Sequential()\n",
    "model_fin.add(layers.Dense(256, activation=\"relu\", input_shape=[X_train.shape[1]]))\n",
    "model_fin.add(layers.Dropout(0.3))\n",
    "model_fin.add(layers.BatchNormalization())\n",
    "for _ in range(4):\n",
    "    model_fin.add(layers.Dense(256, activation=\"relu\"))\n",
    "    model_fin.add(layers.Dropout(0.3))\n",
    "    model_fin.add(layers.BatchNormalization())\n",
    "model_fin.add(layers.Dense(3, activation=\"softmax\"))\n",
    "\n",
    "model_fin.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "history = model_fin.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=512,\n",
    "    epochs=1000,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "y_pred = model_fin.predict(X_val)\n",
    "y_pred = np.round(y_pred)\n",
    "y_pred_class = np.argmax(y_pred, axis=1)\n",
    "class_rate = np.mean(y_pred_class == y_val.values.ravel())\n",
    "print(f\"Classification rate: {class_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "465/465 [==============================] - 2s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>status_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50785</td>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51630</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17168</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45559</td>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49871</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    status_group\n",
       "0  50785  non functional\n",
       "1  51630      functional\n",
       "2  17168      functional\n",
       "3  45559  non functional\n",
       "4  49871      functional"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test data\n",
    "X_test = pd.read_csv('../prep_data/X_test.csv')\n",
    "\n",
    "# Prepare submission\n",
    "output = pd.DataFrame(X_test[\"id\"])\n",
    "X_test.drop(columns=[\"id\"], inplace=True)\n",
    "\n",
    "y_test = model_fin.predict(X_test)\n",
    "y_test = np.round(y_test)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "output[\"status_group\"] = y_test\n",
    "# Map to right strings again\n",
    "output[\"status_group\"] = output[\"status_group\"].map({0: \"non functional\", 1: \"functional needs repair\", 2: \"functional\"})\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "output.to_csv('../submissions/submission_deep_tf.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final note:\n",
    "\n",
    "After submission, the resulting score was 0.7568. This is quite close to the score we got on the validation data here. It is also slightly worse than the XGBoost model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
